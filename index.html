<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Open Voice-Chat Agentic Model Design, Evaluation and Benchmarking - IEEE ASRU 2025</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid #eee;
        }
        h1 {
            color: #0066cc;
            margin-bottom: 10px;
        }
        h2 {
            color: #0066cc;
            margin-top: 30px;
            padding-bottom: 5px;
            border-bottom: 1px solid #eee;
        }
        h3 {
            color: #333;
            margin-top: 20px;
        }
        .organizer {
            margin-bottom: 15px;
        }
        .bio {
            margin-bottom: 25px;
            text-align: justify;
        }
        .topic {
            margin-bottom: 20px;
        }
        .submission-button {
            display: inline-block;
            background-color: #0066cc;
            color: white;
            padding: 12px 25px;
            text-decoration: none;
            border-radius: 5px;
            font-weight: bold;
            margin: 20px 0;
            transition: background-color 0.3s;
        }
        .submission-button:hover {
            background-color: #004999;
        }
        .submission-section {
            text-align: center;
            margin: 40px 0;
            padding: 20px;
            background-color: #f5f5f5;
            border-radius: 5px;
        }
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #eee;
            text-align: center;
            font-size: 0.9em;
            color: #666;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Open Voice-Chat Agentic Model Design, Evaluation and Benchmarking</h1>
        <p>Special Session at IEEE ASRU 2025</p>
    </div>

    <div class="submission-section">
        <h2>Paper Submission</h2>
        <p>Submit your paper to this special session through the IEEE ASRU 2025 submission system:</p>
        <a href="https://cmt3.research.microsoft.com/IEEEASRU2025/Track/1/Submission/Create" class="submission-button">Submit Your Paper</a>
    </div>

    <h2>Organizers and Contact Information</h2>
    <div class="organizer">
        <p><strong>Dr. Huck Yang</strong> – Senior Research Scientist, NVIDIA Research</p>
        <p>Email: huckyang@nvidia.com</p>
    </div>
    <div class="organizer">
        <p><strong>Prof. Yun-Nung (Vivian) Chen</strong> – Professor, Department of Computer Science & Information Engineering, National Taiwan University</p>
        <p>Email: y.v.chen@ieee.org</p>
    </div>
    <div class="organizer">
        <p><strong>Prof. Larry Heck</strong> – Professor (ECE & Interactive Computing), Georgia Institute of Technology; Georgia Research Alliance Eminent Scholar</p>
        <p>Email: larryheck@gatech.edu</p>
    </div>

    <h2>Organizer Biographies</h2>
    <div class="bio">
        <h3>Dr. Huck Yang</h3>
        <p>Dr. Huck Yang is a Senior Research Scientist at NVIDIA Research focusing on speech-language cross-modal alignment with several representative works on input-based model adaptation (i.e., speech-based in-context learning, textual prompting, and system reprogramming.) He received his Ph.D. from the Georgia Institute of Technology and has served as an area chair or committee member on parameter-efficient learning at IEEE ICASSP 2022 to 2025, EMNLP 2024 and ACL 2025​. Dr. Yang has organized special sessions and tutorials in Interspeech 2023, ICASSP 2022 to 2024, and EMNLP 2025.</p>
    </div>
    <div class="bio">
        <h3>Prof. Yun-Nung (Vivian) Chen</h3>
        <p>Prof. Yun-Nung (Vivian) Chen is a Professor in the Department of Computer Science and Information Engineering at National Taiwan University, specializing in spoken dialogue systems, language understanding, and multimodal language processing. She earned her Ph.D. from Carnegie Mellon University. She leads the MiU Lab at NTU, has published extensively on conversational AI, and her expertise in dialog system evaluation and user-centric AI design will inform the session's focus on benchmarking conversational agents.</p>
    </div>
    <div class="bio">
        <h3>Prof. Larry Heck</h3>
        <p>Prof. Larry Heck is a Professor with joint appointments in Electrical & Computer Engineering and Interactive Computing at the Georgia Institute of Technology, and holds the Rhesa S. Farmer Chair of Advanced Computing Concepts as a Georgia Research Alliance Eminent Scholar​. An IEEE Fellow, Prof. Prof. Heck's unparalleled industry experience in creating and evaluating voice assistants makes him ideally suited to co-organize this special session on voice-agent benchmarking.</p>
    </div>

    <h2>Session Overview</h2>
    <p>This special session aims to foster the design of open voice-chat agentic systems and the definition of evaluation benchmarks for such agents. Recent advances in speech technology and large language models (LLMs) have enabled voice-based conversational agents that can engage in free-form dialogue and carry out complex tasks. However, the field currently lacks well-defined tasks and standardized metrics to evaluate these multimodal dialogue systems' performance​.</p>

    <h2>Objectives</h2>
    <p>The primary objective is to define and accelerate the development of open evaluation frameworks for voice-interactive AI agents.</p>

    <h2>Key Topics and Subtopics</h2>
    <p>To structure the session, we will focus on several interconnected subtopics, each crucial for establishing comprehensive benchmarks for voice-chat agents:</p>
    
    <div class="topic">
        <h3>Voice-Chat Model Design</h3>
        <p>Neural network model design for voice-chat function and tasks.</p>
    </div>

    <div class="topic">
        <h3>Real-Time Agent Evaluation</h3>
        <p>Metrics and methods for measuring system latency, turn-taking speed, and real-time interaction quality.</p>
    </div>

    <div class="topic">
        <h3>Benchmarking Frameworks for Voice-Chat Systems</h3>
        <p>Design of open benchmarks and evaluation toolkits that cover the voice-chat capabilities of modern chat agents (speech, text, and possibly vision).</p>
    </div>

    <div class="topic">
        <h3>Integration of Speech Models with LLMs</h3>
        <p>Strategies for large language models to create agentic conversational AI. This topic covers how to evaluate the combined system.</p>
    </div>

    <h2>Session Format</h2>
    <p>The session will be organized as a sequence of invited talks and paper presentations, followed by an interactive panel discussion. We plan to feature 6 to 8 contributed papers (selected via peer review of this special session's submissions). Each paper will present either a novel voice-chat model design, voice-chat task, evaluation methodology, a benchmarking dataset, or an implemented system related to the session theme.</p>

    <h2>Expected Impact</h2>
    <p>By defining currently undefined tasks and metrics for voice-chat agents, this special session will lay the groundwork for standardized evaluations in our community. In the long term, having common benchmarks will accelerate progress by enabling researchers to compare systems directly and track improvements over time. We expect lively participation from both academic researchers and industrial practitioners (e.g., teams working on digital assistants and customer service bots), fostering collaborations that bridge speech and language disciplines. Ultimately, this session aligns with IEEE ASRU's goal of advancing speech understanding technologies: it will help answer "How do we know if one conversational system is better than another?" in a rigorous way. Establishing these evaluation criteria and benchmarks now will shape research and development of next-generation conversational AI that is more accurate, real-time, privacy-preserving, and robust in the open world.</p>

    <div class="submission-section">
        <h2>Submit Your Paper</h2>
        <p>We invite researchers and practitioners to submit their work on voice-chat agentic models, evaluation methods, and benchmarking frameworks.</p>
        <a href="https://cmt3.research.microsoft.com/IEEEASRU2025/Track/1/Submission/Create" class="submission-button">Submit Paper to IEEE ASRU 2025</a>
    </div>

    <div class="footer">
        <p>&copy; 2025 IEEE Automatic Speech Recognition and Understanding Workshop</p>
    </div>
</body>
</html> 